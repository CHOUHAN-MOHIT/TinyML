{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "g84rsKY9Bl8c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = tf.keras.datasets.fashion_mnist"
      ],
      "metadata": {
        "id": "8flPenXCGgkP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(training_images, training_labels), (val_images, val_labels) = data.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0Z-ykTGGjwO",
        "outputId": "ebd98c45-c686-4ae5-fa69-34cfe7bb69a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of training images:\", training_images.shape)\n",
        "print(\"Shape of training labels:\", training_labels.shape)\n",
        "print(\"Shape of testing images :\", val_images.shape)\n",
        "print(\"Shape of testing labels :\", val_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw4hsc6ZGjtm",
        "outputId": "b1482319-6e6e-4059-fbe5-7347669cae35"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training images: (60000, 28, 28)\n",
            "Shape of training labels: (60000,)\n",
            "Shape of testing images : (10000, 28, 28)\n",
            "Shape of testing labels : (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Images are stored as matrices\n",
        "print(training_images[0])\n",
        "print()\n",
        "print(\"Class:\", training_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fR-fh3LqGjrQ",
        "outputId": "e74019e0-e9dc-4d41-eab5-24f38b4b48e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n",
            "    0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n",
            "   54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n",
            "  144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n",
            "  107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n",
            "  216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n",
            "  223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
            "  235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
            "  180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n",
            "  169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n",
            "  198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n",
            "  232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n",
            "  222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
            "  211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n",
            "  224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n",
            "  255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
            "  188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n",
            "  168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n",
            "  239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n",
            "  199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n",
            "  195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n",
            "  210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n",
            "  182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n",
            "\n",
            "Class: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot image\n",
        "plt.imshow(training_images[0], cmap='gray')\n",
        "print(\"Class:\", training_labels[0])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "iM-5T1YuGjnG",
        "outputId": "8cee3567-da76-4e87-f7a2-312ea75418c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: 9\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_images  = training_images / 255.0\n",
        "val_images = val_images / 255.0"
      ],
      "metadata": {
        "id": "OGtuNL0SGjkH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_images[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UkukSHPzGjdf",
        "outputId": "ae53160e-a79e-4141-b9ad-cf39959fd288"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.00392157 0.         0.         0.05098039 0.28627451 0.\n",
            "  0.         0.00392157 0.01568627 0.         0.         0.\n",
            "  0.         0.00392157 0.00392157 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.01176471 0.         0.14117647 0.53333333 0.49803922 0.24313725\n",
            "  0.21176471 0.         0.         0.         0.00392157 0.01176471\n",
            "  0.01568627 0.         0.         0.01176471]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.02352941 0.         0.4        0.8        0.69019608 0.5254902\n",
            "  0.56470588 0.48235294 0.09019608 0.         0.         0.\n",
            "  0.         0.04705882 0.03921569 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.60784314 0.9254902  0.81176471 0.69803922\n",
            "  0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608\n",
            "  0.30196078 0.50980392 0.28235294 0.05882353]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.00392157\n",
            "  0.         0.27058824 0.81176471 0.8745098  0.85490196 0.84705882\n",
            "  0.84705882 0.63921569 0.49803922 0.4745098  0.47843137 0.57254902\n",
            "  0.55294118 0.34509804 0.6745098  0.25882353]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.00392157 0.00392157 0.00392157\n",
            "  0.         0.78431373 0.90980392 0.90980392 0.91372549 0.89803922\n",
            "  0.8745098  0.8745098  0.84313725 0.83529412 0.64313725 0.49803922\n",
            "  0.48235294 0.76862745 0.89803922 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.71764706 0.88235294 0.84705882 0.8745098  0.89411765\n",
            "  0.92156863 0.89019608 0.87843137 0.87058824 0.87843137 0.86666667\n",
            "  0.8745098  0.96078431 0.67843137 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.75686275 0.89411765 0.85490196 0.83529412 0.77647059\n",
            "  0.70588235 0.83137255 0.82352941 0.82745098 0.83529412 0.8745098\n",
            "  0.8627451  0.95294118 0.79215686 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.00392157 0.01176471 0.\n",
            "  0.04705882 0.85882353 0.8627451  0.83137255 0.85490196 0.75294118\n",
            "  0.6627451  0.89019608 0.81568627 0.85490196 0.87843137 0.83137255\n",
            "  0.88627451 0.77254902 0.81960784 0.20392157]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.02352941 0.\n",
            "  0.38823529 0.95686275 0.87058824 0.8627451  0.85490196 0.79607843\n",
            "  0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451\n",
            "  0.96078431 0.46666667 0.65490196 0.21960784]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.01568627 0.         0.\n",
            "  0.21568627 0.9254902  0.89411765 0.90196078 0.89411765 0.94117647\n",
            "  0.90980392 0.83529412 0.85490196 0.8745098  0.91764706 0.85098039\n",
            "  0.85098039 0.81960784 0.36078431 0.        ]\n",
            " [0.         0.         0.00392157 0.01568627 0.02352941 0.02745098\n",
            "  0.00784314 0.         0.         0.         0.         0.\n",
            "  0.92941176 0.88627451 0.85098039 0.8745098  0.87058824 0.85882353\n",
            "  0.87058824 0.86666667 0.84705882 0.8745098  0.89803922 0.84313725\n",
            "  0.85490196 1.         0.30196078 0.        ]\n",
            " [0.         0.01176471 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.24313725 0.56862745 0.8\n",
            "  0.89411765 0.81176471 0.83529412 0.86666667 0.85490196 0.81568627\n",
            "  0.82745098 0.85490196 0.87843137 0.8745098  0.85882353 0.84313725\n",
            "  0.87843137 0.95686275 0.62352941 0.        ]\n",
            " [0.         0.         0.         0.         0.07058824 0.17254902\n",
            "  0.32156863 0.41960784 0.74117647 0.89411765 0.8627451  0.87058824\n",
            "  0.85098039 0.88627451 0.78431373 0.80392157 0.82745098 0.90196078\n",
            "  0.87843137 0.91764706 0.69019608 0.7372549  0.98039216 0.97254902\n",
            "  0.91372549 0.93333333 0.84313725 0.        ]\n",
            " [0.         0.22352941 0.73333333 0.81568627 0.87843137 0.86666667\n",
            "  0.87843137 0.81568627 0.8        0.83921569 0.81568627 0.81960784\n",
            "  0.78431373 0.62352941 0.96078431 0.75686275 0.80784314 0.8745098\n",
            "  1.         1.         0.86666667 0.91764706 0.86666667 0.82745098\n",
            "  0.8627451  0.90980392 0.96470588 0.        ]\n",
            " [0.01176471 0.79215686 0.89411765 0.87843137 0.86666667 0.82745098\n",
            "  0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451\n",
            "  0.94117647 0.31372549 0.58823529 1.         0.89803922 0.86666667\n",
            "  0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784\n",
            "  0.87058824 0.89411765 0.88235294 0.        ]\n",
            " [0.38431373 0.91372549 0.77647059 0.82352941 0.87058824 0.89803922\n",
            "  0.89803922 0.91764706 0.97647059 0.8627451  0.76078431 0.84313725\n",
            "  0.85098039 0.94509804 0.25490196 0.28627451 0.41568627 0.45882353\n",
            "  0.65882353 0.85882353 0.86666667 0.84313725 0.85098039 0.8745098\n",
            "  0.8745098  0.87843137 0.89803922 0.11372549]\n",
            " [0.29411765 0.8        0.83137255 0.8        0.75686275 0.80392157\n",
            "  0.82745098 0.88235294 0.84705882 0.7254902  0.77254902 0.80784314\n",
            "  0.77647059 0.83529412 0.94117647 0.76470588 0.89019608 0.96078431\n",
            "  0.9372549  0.8745098  0.85490196 0.83137255 0.81960784 0.87058824\n",
            "  0.8627451  0.86666667 0.90196078 0.2627451 ]\n",
            " [0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902\n",
            "  0.7254902  0.74509804 0.76078431 0.75294118 0.79215686 0.83921569\n",
            "  0.85882353 0.86666667 0.8627451  0.9254902  0.88235294 0.84705882\n",
            "  0.78039216 0.80784314 0.72941176 0.70980392 0.69411765 0.6745098\n",
            "  0.70980392 0.80392157 0.80784314 0.45098039]\n",
            " [0.         0.47843137 0.85882353 0.75686275 0.70196078 0.67058824\n",
            "  0.71764706 0.76862745 0.8        0.82352941 0.83529412 0.81176471\n",
            "  0.82745098 0.82352941 0.78431373 0.76862745 0.76078431 0.74901961\n",
            "  0.76470588 0.74901961 0.77647059 0.75294118 0.69019608 0.61176471\n",
            "  0.65490196 0.69411765 0.82352941 0.36078431]\n",
            " [0.         0.         0.29019608 0.74117647 0.83137255 0.74901961\n",
            "  0.68627451 0.6745098  0.68627451 0.70980392 0.7254902  0.7372549\n",
            "  0.74117647 0.7372549  0.75686275 0.77647059 0.8        0.81960784\n",
            "  0.82352941 0.82352941 0.82745098 0.7372549  0.7372549  0.76078431\n",
            "  0.75294118 0.84705882 0.66666667 0.        ]\n",
            " [0.00784314 0.         0.         0.         0.25882353 0.78431373\n",
            "  0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118\n",
            "  0.95686275 0.86666667 0.8627451  0.75686275 0.74901961 0.70196078\n",
            "  0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353\n",
            "  0.38823529 0.22745098 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.15686275 0.23921569 0.17254902 0.28235294 0.16078431\n",
            "  0.1372549  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Flatten the 28x28 images into a single 784-dimensional vector\n",
        "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
        "    Dense(64, activation='relu'),   # Second hidden layer with 64 neurons and ReLU activation\n",
        "    Dense(10, activation='softmax') # Output layer with 10 neurons (one for each class) and softmax activation\n",
        "])"
      ],
      "metadata": {
        "id": "2JqB8iC7GjPO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPT = 'sgd'\n",
        "LOSS = 'mse'\n",
        "\n",
        "model.compile(optimizer= OPT,\n",
        "              loss= LOSS,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "xU5dS4KuGzjQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(training_images, training_labels, epochs=200, validation_data=(val_images, val_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BISjYbTxGzg-",
        "outputId": "67150825-610d-4332-e038-503cf8fc9702"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 27.6105 - accuracy: 0.1241 - val_loss: 27.6103 - val_accuracy: 0.1070\n",
            "Epoch 2/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6102 - accuracy: 0.1040 - val_loss: 27.6102 - val_accuracy: 0.1027\n",
            "Epoch 3/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6102 - accuracy: 0.1075 - val_loss: 27.6102 - val_accuracy: 0.1086\n",
            "Epoch 4/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6102 - accuracy: 0.1129 - val_loss: 27.6102 - val_accuracy: 0.1127\n",
            "Epoch 5/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6102 - accuracy: 0.1171 - val_loss: 27.6102 - val_accuracy: 0.1165\n",
            "Epoch 6/200\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 27.6102 - accuracy: 0.1200 - val_loss: 27.6101 - val_accuracy: 0.1199\n",
            "Epoch 7/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1219 - val_loss: 27.6101 - val_accuracy: 0.1249\n",
            "Epoch 8/200\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 27.6102 - accuracy: 0.1235 - val_loss: 27.6101 - val_accuracy: 0.1265\n",
            "Epoch 9/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6102 - accuracy: 0.1241 - val_loss: 27.6101 - val_accuracy: 0.1270\n",
            "Epoch 10/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1245 - val_loss: 27.6101 - val_accuracy: 0.1283\n",
            "Epoch 11/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1260 - val_loss: 27.6101 - val_accuracy: 0.1287\n",
            "Epoch 12/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6102 - accuracy: 0.1271 - val_loss: 27.6101 - val_accuracy: 0.1293\n",
            "Epoch 13/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1274 - val_loss: 27.6101 - val_accuracy: 0.1306\n",
            "Epoch 14/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6102 - accuracy: 0.1279 - val_loss: 27.6101 - val_accuracy: 0.1300\n",
            "Epoch 15/200\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 27.6101 - accuracy: 0.1280 - val_loss: 27.6101 - val_accuracy: 0.1301\n",
            "Epoch 16/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1282 - val_loss: 27.6101 - val_accuracy: 0.1291\n",
            "Epoch 17/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6102 - accuracy: 0.1286 - val_loss: 27.6101 - val_accuracy: 0.1298\n",
            "Epoch 18/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1284 - val_loss: 27.6101 - val_accuracy: 0.1300\n",
            "Epoch 19/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1286 - val_loss: 27.6101 - val_accuracy: 0.1304\n",
            "Epoch 20/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1285 - val_loss: 27.6101 - val_accuracy: 0.1296\n",
            "Epoch 21/200\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 27.6101 - accuracy: 0.1280 - val_loss: 27.6101 - val_accuracy: 0.1308\n",
            "Epoch 22/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1278 - val_loss: 27.6101 - val_accuracy: 0.1303\n",
            "Epoch 23/200\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 27.6101 - accuracy: 0.1280 - val_loss: 27.6101 - val_accuracy: 0.1311\n",
            "Epoch 24/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1280 - val_loss: 27.6101 - val_accuracy: 0.1302\n",
            "Epoch 25/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1283 - val_loss: 27.6101 - val_accuracy: 0.1304\n",
            "Epoch 26/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1280 - val_loss: 27.6101 - val_accuracy: 0.1302\n",
            "Epoch 27/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1278 - val_loss: 27.6101 - val_accuracy: 0.1303\n",
            "Epoch 28/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1276 - val_loss: 27.6101 - val_accuracy: 0.1296\n",
            "Epoch 29/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1276 - val_loss: 27.6101 - val_accuracy: 0.1291\n",
            "Epoch 30/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1279 - val_loss: 27.6101 - val_accuracy: 0.1297\n",
            "Epoch 31/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1277 - val_loss: 27.6101 - val_accuracy: 0.1295\n",
            "Epoch 32/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1278 - val_loss: 27.6101 - val_accuracy: 0.1296\n",
            "Epoch 33/200\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 27.6101 - accuracy: 0.1279 - val_loss: 27.6101 - val_accuracy: 0.1287\n",
            "Epoch 34/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1279 - val_loss: 27.6101 - val_accuracy: 0.1287\n",
            "Epoch 35/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1282 - val_loss: 27.6101 - val_accuracy: 0.1290\n",
            "Epoch 36/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1284 - val_loss: 27.6101 - val_accuracy: 0.1280\n",
            "Epoch 37/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1285 - val_loss: 27.6101 - val_accuracy: 0.1285\n",
            "Epoch 38/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1291 - val_loss: 27.6101 - val_accuracy: 0.1287\n",
            "Epoch 39/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1291 - val_loss: 27.6101 - val_accuracy: 0.1289\n",
            "Epoch 40/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1298 - val_loss: 27.6101 - val_accuracy: 0.1292\n",
            "Epoch 41/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1294 - val_loss: 27.6100 - val_accuracy: 0.1292\n",
            "Epoch 42/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1294 - val_loss: 27.6100 - val_accuracy: 0.1293\n",
            "Epoch 43/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1292 - val_loss: 27.6100 - val_accuracy: 0.1299\n",
            "Epoch 44/200\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 27.6101 - accuracy: 0.1293 - val_loss: 27.6100 - val_accuracy: 0.1310\n",
            "Epoch 45/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1295 - val_loss: 27.6100 - val_accuracy: 0.1308\n",
            "Epoch 46/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1295 - val_loss: 27.6100 - val_accuracy: 0.1306\n",
            "Epoch 47/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1299 - val_loss: 27.6100 - val_accuracy: 0.1312\n",
            "Epoch 48/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1303 - val_loss: 27.6100 - val_accuracy: 0.1303\n",
            "Epoch 49/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1305 - val_loss: 27.6100 - val_accuracy: 0.1298\n",
            "Epoch 50/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1311 - val_loss: 27.6100 - val_accuracy: 0.1309\n",
            "Epoch 51/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1310 - val_loss: 27.6100 - val_accuracy: 0.1294\n",
            "Epoch 52/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1313 - val_loss: 27.6100 - val_accuracy: 0.1295\n",
            "Epoch 53/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1315 - val_loss: 27.6100 - val_accuracy: 0.1281\n",
            "Epoch 54/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1321 - val_loss: 27.6100 - val_accuracy: 0.1278\n",
            "Epoch 55/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1322 - val_loss: 27.6100 - val_accuracy: 0.1292\n",
            "Epoch 56/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1323 - val_loss: 27.6100 - val_accuracy: 0.1294\n",
            "Epoch 57/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1327 - val_loss: 27.6100 - val_accuracy: 0.1284\n",
            "Epoch 58/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1325 - val_loss: 27.6100 - val_accuracy: 0.1290\n",
            "Epoch 59/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1323 - val_loss: 27.6100 - val_accuracy: 0.1293\n",
            "Epoch 60/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1324 - val_loss: 27.6100 - val_accuracy: 0.1289\n",
            "Epoch 61/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1321 - val_loss: 27.6100 - val_accuracy: 0.1288\n",
            "Epoch 62/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1319 - val_loss: 27.6100 - val_accuracy: 0.1281\n",
            "Epoch 63/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1319 - val_loss: 27.6100 - val_accuracy: 0.1272\n",
            "Epoch 64/200\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 27.6101 - accuracy: 0.1318 - val_loss: 27.6100 - val_accuracy: 0.1275\n",
            "Epoch 65/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1314 - val_loss: 27.6100 - val_accuracy: 0.1258\n",
            "Epoch 66/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1308 - val_loss: 27.6100 - val_accuracy: 0.1257\n",
            "Epoch 67/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1309 - val_loss: 27.6100 - val_accuracy: 0.1254\n",
            "Epoch 68/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1304 - val_loss: 27.6100 - val_accuracy: 0.1245\n",
            "Epoch 69/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1298 - val_loss: 27.6100 - val_accuracy: 0.1247\n",
            "Epoch 70/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1294 - val_loss: 27.6100 - val_accuracy: 0.1249\n",
            "Epoch 71/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1288 - val_loss: 27.6100 - val_accuracy: 0.1249\n",
            "Epoch 72/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1288 - val_loss: 27.6100 - val_accuracy: 0.1250\n",
            "Epoch 73/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1284 - val_loss: 27.6100 - val_accuracy: 0.1253\n",
            "Epoch 74/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1283 - val_loss: 27.6100 - val_accuracy: 0.1258\n",
            "Epoch 75/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1283 - val_loss: 27.6100 - val_accuracy: 0.1257\n",
            "Epoch 76/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1281 - val_loss: 27.6100 - val_accuracy: 0.1258\n",
            "Epoch 77/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1280 - val_loss: 27.6100 - val_accuracy: 0.1260\n",
            "Epoch 78/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1278 - val_loss: 27.6100 - val_accuracy: 0.1259\n",
            "Epoch 79/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1273 - val_loss: 27.6100 - val_accuracy: 0.1264\n",
            "Epoch 80/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1269 - val_loss: 27.6100 - val_accuracy: 0.1261\n",
            "Epoch 81/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1269 - val_loss: 27.6100 - val_accuracy: 0.1255\n",
            "Epoch 82/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1268 - val_loss: 27.6100 - val_accuracy: 0.1257\n",
            "Epoch 83/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1265 - val_loss: 27.6100 - val_accuracy: 0.1262\n",
            "Epoch 84/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1266 - val_loss: 27.6100 - val_accuracy: 0.1258\n",
            "Epoch 85/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1267 - val_loss: 27.6100 - val_accuracy: 0.1252\n",
            "Epoch 86/200\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 27.6101 - accuracy: 0.1267 - val_loss: 27.6100 - val_accuracy: 0.1256\n",
            "Epoch 87/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1267 - val_loss: 27.6100 - val_accuracy: 0.1253\n",
            "Epoch 88/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1268 - val_loss: 27.6100 - val_accuracy: 0.1257\n",
            "Epoch 89/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1267 - val_loss: 27.6100 - val_accuracy: 0.1256\n",
            "Epoch 90/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1267 - val_loss: 27.6100 - val_accuracy: 0.1252\n",
            "Epoch 91/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1267 - val_loss: 27.6100 - val_accuracy: 0.1249\n",
            "Epoch 92/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1268 - val_loss: 27.6100 - val_accuracy: 0.1249\n",
            "Epoch 93/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1268 - val_loss: 27.6100 - val_accuracy: 0.1255\n",
            "Epoch 94/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1268 - val_loss: 27.6100 - val_accuracy: 0.1258\n",
            "Epoch 95/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1269 - val_loss: 27.6100 - val_accuracy: 0.1260\n",
            "Epoch 96/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1272 - val_loss: 27.6100 - val_accuracy: 0.1261\n",
            "Epoch 97/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1272 - val_loss: 27.6100 - val_accuracy: 0.1255\n",
            "Epoch 98/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1273 - val_loss: 27.6100 - val_accuracy: 0.1254\n",
            "Epoch 99/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1272 - val_loss: 27.6100 - val_accuracy: 0.1258\n",
            "Epoch 100/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1271 - val_loss: 27.6100 - val_accuracy: 0.1263\n",
            "Epoch 101/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1273 - val_loss: 27.6100 - val_accuracy: 0.1267\n",
            "Epoch 102/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1273 - val_loss: 27.6100 - val_accuracy: 0.1266\n",
            "Epoch 103/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1275 - val_loss: 27.6100 - val_accuracy: 0.1267\n",
            "Epoch 104/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1278 - val_loss: 27.6100 - val_accuracy: 0.1266\n",
            "Epoch 105/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1281 - val_loss: 27.6100 - val_accuracy: 0.1270\n",
            "Epoch 106/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1278 - val_loss: 27.6100 - val_accuracy: 0.1271\n",
            "Epoch 107/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1281 - val_loss: 27.6100 - val_accuracy: 0.1270\n",
            "Epoch 108/200\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 27.6101 - accuracy: 0.1280 - val_loss: 27.6100 - val_accuracy: 0.1277\n",
            "Epoch 109/200\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 27.6101 - accuracy: 0.1276 - val_loss: 27.6100 - val_accuracy: 0.1277\n",
            "Epoch 110/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1276 - val_loss: 27.6100 - val_accuracy: 0.1278\n",
            "Epoch 111/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1276 - val_loss: 27.6100 - val_accuracy: 0.1277\n",
            "Epoch 112/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1275 - val_loss: 27.6100 - val_accuracy: 0.1278\n",
            "Epoch 113/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1277 - val_loss: 27.6100 - val_accuracy: 0.1282\n",
            "Epoch 114/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1280 - val_loss: 27.6100 - val_accuracy: 0.1286\n",
            "Epoch 115/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1282 - val_loss: 27.6100 - val_accuracy: 0.1298\n",
            "Epoch 116/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1280 - val_loss: 27.6100 - val_accuracy: 0.1301\n",
            "Epoch 117/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1284 - val_loss: 27.6100 - val_accuracy: 0.1305\n",
            "Epoch 118/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1285 - val_loss: 27.6100 - val_accuracy: 0.1299\n",
            "Epoch 119/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1287 - val_loss: 27.6100 - val_accuracy: 0.1294\n",
            "Epoch 120/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1289 - val_loss: 27.6100 - val_accuracy: 0.1306\n",
            "Epoch 121/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1292 - val_loss: 27.6100 - val_accuracy: 0.1309\n",
            "Epoch 122/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1296 - val_loss: 27.6100 - val_accuracy: 0.1311\n",
            "Epoch 123/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1297 - val_loss: 27.6100 - val_accuracy: 0.1319\n",
            "Epoch 124/200\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 27.6101 - accuracy: 0.1302 - val_loss: 27.6100 - val_accuracy: 0.1329\n",
            "Epoch 125/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1304 - val_loss: 27.6100 - val_accuracy: 0.1343\n",
            "Epoch 126/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1309 - val_loss: 27.6100 - val_accuracy: 0.1343\n",
            "Epoch 127/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1312 - val_loss: 27.6100 - val_accuracy: 0.1346\n",
            "Epoch 128/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1317 - val_loss: 27.6100 - val_accuracy: 0.1347\n",
            "Epoch 129/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1323 - val_loss: 27.6100 - val_accuracy: 0.1352\n",
            "Epoch 130/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1328 - val_loss: 27.6100 - val_accuracy: 0.1351\n",
            "Epoch 131/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1333 - val_loss: 27.6100 - val_accuracy: 0.1348\n",
            "Epoch 132/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1337 - val_loss: 27.6100 - val_accuracy: 0.1357\n",
            "Epoch 133/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1341 - val_loss: 27.6100 - val_accuracy: 0.1358\n",
            "Epoch 134/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1341 - val_loss: 27.6100 - val_accuracy: 0.1366\n",
            "Epoch 135/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1342 - val_loss: 27.6100 - val_accuracy: 0.1384\n",
            "Epoch 136/200\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 27.6101 - accuracy: 0.1347 - val_loss: 27.6100 - val_accuracy: 0.1382\n",
            "Epoch 137/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1347 - val_loss: 27.6100 - val_accuracy: 0.1391\n",
            "Epoch 138/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1348 - val_loss: 27.6100 - val_accuracy: 0.1392\n",
            "Epoch 139/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1347 - val_loss: 27.6100 - val_accuracy: 0.1393\n",
            "Epoch 140/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1350 - val_loss: 27.6100 - val_accuracy: 0.1394\n",
            "Epoch 141/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1350 - val_loss: 27.6100 - val_accuracy: 0.1390\n",
            "Epoch 142/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1350 - val_loss: 27.6100 - val_accuracy: 0.1390\n",
            "Epoch 143/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1354 - val_loss: 27.6100 - val_accuracy: 0.1378\n",
            "Epoch 144/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1353 - val_loss: 27.6100 - val_accuracy: 0.1376\n",
            "Epoch 145/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1355 - val_loss: 27.6100 - val_accuracy: 0.1385\n",
            "Epoch 146/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1355 - val_loss: 27.6100 - val_accuracy: 0.1394\n",
            "Epoch 147/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1355 - val_loss: 27.6100 - val_accuracy: 0.1391\n",
            "Epoch 148/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1351 - val_loss: 27.6100 - val_accuracy: 0.1397\n",
            "Epoch 149/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1349 - val_loss: 27.6100 - val_accuracy: 0.1393\n",
            "Epoch 150/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1349 - val_loss: 27.6100 - val_accuracy: 0.1387\n",
            "Epoch 151/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1349 - val_loss: 27.6100 - val_accuracy: 0.1384\n",
            "Epoch 152/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1345 - val_loss: 27.6100 - val_accuracy: 0.1380\n",
            "Epoch 153/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1345 - val_loss: 27.6100 - val_accuracy: 0.1377\n",
            "Epoch 154/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1340 - val_loss: 27.6100 - val_accuracy: 0.1371\n",
            "Epoch 155/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1335 - val_loss: 27.6100 - val_accuracy: 0.1363\n",
            "Epoch 156/200\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 27.6101 - accuracy: 0.1332 - val_loss: 27.6100 - val_accuracy: 0.1353\n",
            "Epoch 157/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1329 - val_loss: 27.6100 - val_accuracy: 0.1349\n",
            "Epoch 158/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1327 - val_loss: 27.6100 - val_accuracy: 0.1342\n",
            "Epoch 159/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1322 - val_loss: 27.6100 - val_accuracy: 0.1332\n",
            "Epoch 160/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1319 - val_loss: 27.6100 - val_accuracy: 0.1327\n",
            "Epoch 161/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1317 - val_loss: 27.6100 - val_accuracy: 0.1324\n",
            "Epoch 162/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1313 - val_loss: 27.6100 - val_accuracy: 0.1324\n",
            "Epoch 163/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1316 - val_loss: 27.6100 - val_accuracy: 0.1325\n",
            "Epoch 164/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1315 - val_loss: 27.6100 - val_accuracy: 0.1318\n",
            "Epoch 165/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1312 - val_loss: 27.6100 - val_accuracy: 0.1320\n",
            "Epoch 166/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1310 - val_loss: 27.6100 - val_accuracy: 0.1317\n",
            "Epoch 167/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1308 - val_loss: 27.6100 - val_accuracy: 0.1314\n",
            "Epoch 168/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1305 - val_loss: 27.6100 - val_accuracy: 0.1317\n",
            "Epoch 169/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1296 - val_loss: 27.6100 - val_accuracy: 0.1312\n",
            "Epoch 170/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1297 - val_loss: 27.6100 - val_accuracy: 0.1308\n",
            "Epoch 171/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1295 - val_loss: 27.6100 - val_accuracy: 0.1305\n",
            "Epoch 172/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1294 - val_loss: 27.6100 - val_accuracy: 0.1302\n",
            "Epoch 173/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1293 - val_loss: 27.6100 - val_accuracy: 0.1296\n",
            "Epoch 174/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1291 - val_loss: 27.6100 - val_accuracy: 0.1296\n",
            "Epoch 175/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1288 - val_loss: 27.6100 - val_accuracy: 0.1289\n",
            "Epoch 176/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1286 - val_loss: 27.6100 - val_accuracy: 0.1283\n",
            "Epoch 177/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1283 - val_loss: 27.6100 - val_accuracy: 0.1280\n",
            "Epoch 178/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1276 - val_loss: 27.6100 - val_accuracy: 0.1278\n",
            "Epoch 179/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1274 - val_loss: 27.6100 - val_accuracy: 0.1271\n",
            "Epoch 180/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1270 - val_loss: 27.6100 - val_accuracy: 0.1268\n",
            "Epoch 181/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1269 - val_loss: 27.6100 - val_accuracy: 0.1274\n",
            "Epoch 182/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1270 - val_loss: 27.6100 - val_accuracy: 0.1274\n",
            "Epoch 183/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1268 - val_loss: 27.6100 - val_accuracy: 0.1281\n",
            "Epoch 184/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1268 - val_loss: 27.6100 - val_accuracy: 0.1274\n",
            "Epoch 185/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1265 - val_loss: 27.6100 - val_accuracy: 0.1274\n",
            "Epoch 186/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1262 - val_loss: 27.6100 - val_accuracy: 0.1265\n",
            "Epoch 187/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1259 - val_loss: 27.6100 - val_accuracy: 0.1262\n",
            "Epoch 188/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1256 - val_loss: 27.6100 - val_accuracy: 0.1257\n",
            "Epoch 189/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1253 - val_loss: 27.6100 - val_accuracy: 0.1260\n",
            "Epoch 190/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1251 - val_loss: 27.6100 - val_accuracy: 0.1259\n",
            "Epoch 191/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1252 - val_loss: 27.6100 - val_accuracy: 0.1259\n",
            "Epoch 192/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1252 - val_loss: 27.6100 - val_accuracy: 0.1257\n",
            "Epoch 193/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1249 - val_loss: 27.6100 - val_accuracy: 0.1260\n",
            "Epoch 194/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1247 - val_loss: 27.6100 - val_accuracy: 0.1261\n",
            "Epoch 195/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1245 - val_loss: 27.6100 - val_accuracy: 0.1258\n",
            "Epoch 196/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1245 - val_loss: 27.6100 - val_accuracy: 0.1256\n",
            "Epoch 197/200\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 27.6101 - accuracy: 0.1242 - val_loss: 27.6100 - val_accuracy: 0.1255\n",
            "Epoch 198/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1240 - val_loss: 27.6100 - val_accuracy: 0.1248\n",
            "Epoch 199/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 27.6101 - accuracy: 0.1241 - val_loss: 27.6100 - val_accuracy: 0.1246\n",
            "Epoch 200/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 27.6101 - accuracy: 0.1239 - val_loss: 27.6100 - val_accuracy: 0.1245\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f57f21f3b80>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-XJ8MdqGzeW",
        "outputId": "f4c57629-373d-448f-b701-8f6b0a88b221"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109386 (427.29 KB)\n",
            "Trainable params: 109386 (427.29 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S4eBUS2zGzbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_6e-_Un6GzYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bB7fZZZdGzS3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}